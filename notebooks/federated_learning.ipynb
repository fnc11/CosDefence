{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9dbe42e",
   "metadata": {},
   "source": [
    "# Steps\n",
    "1. Define Model\n",
    "2. Pass copies of this Model parameters to Clients\n",
    "3. Clients run x epochs on this model on their data\n",
    "4. Send back the model parameters\n",
    "5. Aggregate (FedAVG) the model parameters\n",
    "6. Repeat from step 2 for y rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1af6b1",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf9d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "from numpy.random import default_rng\n",
    "\n",
    "from collections import OrderedDict, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e372c",
   "metadata": {},
   "source": [
    "## Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c333c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicNet, self).__init__()\n",
    "        # conv layers 1,2\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # FC layers 1, 2, after Max pooling applied 3 times the size will be 4x4x128\n",
    "        self.fc1 = nn.Linear(4*4*128, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        # drop out layer with p=0.2\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # passing through Convolution and max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # flattening the image\n",
    "        x = x.view(-1, 4*4*128)\n",
    "        # drop out layer\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        # final class scores are sent as it is\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364f2131",
   "metadata": {},
   "source": [
    "## ResNet Model Pre-trained\n",
    "We'll freeze other layer except the last 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5646b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_model_pre = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bcda3bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18_model_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "440e5b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first freezing all params\n",
    "for param in resnet18_model_pre.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# replacing all last fc layer with the classifier, which\n",
    "# itself consist of 2 layers, 'final_layer' is our final layer now\n",
    "classifier = nn.Sequential(OrderedDict({\n",
    "    'fc1' : nn.Linear(512, 256),\n",
    "    'relu1': nn.ReLU(),\n",
    "    'final_layer' : nn.Linear(256, 10)\n",
    "}))\n",
    "\n",
    "resnet18_model_pre.fc = classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1951730e",
   "metadata": {},
   "source": [
    "### Resnet model Raw\n",
    "Without Pre-trained params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2159dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_model = models.resnet18(pretrained=False)\n",
    "resnet18_model.fc = nn.Sequential(OrderedDict({'final_layer' : nn.Linear(512, 10)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc44bfc7",
   "metadata": {},
   "source": [
    "## Simulation of Client Training and Server Model Aggregation\n",
    "\n",
    "\n",
    "### Defining Client Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2147175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ClientDataset(Dataset):\n",
    "    def __init__(self, img_tensors, lbl_tensors, transform=None):\n",
    "        self.img_tensors = img_tensors\n",
    "        self.lbl_tensors = lbl_tensors\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.lbl_tensors.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.img_tensors[idx], self.lbl_tensors[idx] \n",
    "    \n",
    "def create_client_data_loaders(total_clients, data_folder, batch_size, random_mode=False):\n",
    "    data_loaders = []\n",
    "    for idx in range(total_clients):\n",
    "        # loading data to tensors\n",
    "        img_tensor_file = data_folder + f'client_{idx}_img.pt'\n",
    "        lbl_tensor_file = data_folder + f'client_{idx}_lbl.pt'\n",
    "        img_tensors = torch.load(img_tensor_file) # this contains 494 images, currently 76\n",
    "        lbl_tensors = torch.load(lbl_tensor_file)\n",
    "\n",
    "        # creating a dataset which can be fed to dataloader\n",
    "        client_dataset = ClientDataset(img_tensors, lbl_tensors)\n",
    "        data_loaders.append(DataLoader(client_dataset, batch_size=batch_size, shuffle=random_mode))\n",
    "    return data_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ece86",
   "metadata": {},
   "source": [
    "### Client Training Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e58cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_client(idx, model, data_loader, optimizer, loss_fn, local_epochs, device):\n",
    "    model.train()\n",
    "    client_training_losses = []\n",
    "    for epoch in range(local_epochs):\n",
    "        train_loss = 0.0\n",
    "        for data, target in data_loader:\n",
    "            # move tensors to GPU device if CUDA is available\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = loss_fn(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        epoch_train_loss = train_loss/len(data_loader)\n",
    "        client_training_losses.append(epoch_train_loss)\n",
    "        print('Client: {}\\t Epoch: {} \\tTraining Loss: {:.6f}'.format(idx, epoch, epoch_train_loss))\n",
    "    return client_training_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c5fc3f",
   "metadata": {},
   "source": [
    "### Server Aggregation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77e27944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fed_avg(server_model, client_models, client_weights):\n",
    "    # Safety lock, to not update model params accidentally\n",
    "    with torch.no_grad():\n",
    "        # need to take avg key-wise\n",
    "        for key in server_model.state_dict().keys():\n",
    "            temp = torch.zeros_like(server_model.state_dict()[key], dtype=torch.float32)\n",
    "            for idx in range(len(client_weights)):\n",
    "                temp += client_weights[idx]*client_models[idx].state_dict()[key]\n",
    "            # update the new value of this key in the server model\n",
    "            server_model.state_dict()[key].data.copy_(temp)\n",
    "            # update this key value in all the client models as well\n",
    "            for idx in range(len(client_weights)):\n",
    "                client_models[idx].state_dict()[key].data.copy_(temp)\n",
    "    return server_model, client_models\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79fa855",
   "metadata": {},
   "source": [
    "### Testing the server model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "777dd104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model, test_loader, loss_fn, device):\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    # specify the image classes\n",
    "    classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "    model.eval()\n",
    "    # iterate over test data\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = loss_fn(output, target)\n",
    "        # update test loss \n",
    "        test_loss += loss.item()\n",
    "        # convert output probabilities to predicted class\n",
    "        top_p, pred_class = torch.max(output, 1)    \n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred_class.eq(target.data.view_as(pred_class))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        # calculate test accuracy for each object class\n",
    "        for i in range(len(target)):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "    # average test loss\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "    \n",
    "    for i in range(10):\n",
    "        if class_total[i] > 0:\n",
    "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "                classes[i], (100 * class_correct[i]) / class_total[i],\n",
    "                np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "        else:\n",
    "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "        100. * np.sum(class_correct) / np.sum(class_total),\n",
    "        np.sum(class_correct), np.sum(class_total)))\n",
    "    test_acc = np.sum(class_correct) / np.sum(class_total)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "# run_test(server_model, test_loader, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a4b04e",
   "metadata": {},
   "source": [
    "## Main Configuration Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc55a788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Device:cuda:0\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Computing Device:{device}\")\n",
    "seed = 42\n",
    "rng = default_rng(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)     \n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# specify which model to use\n",
    "server_model = resnet18_model_pre\n",
    "# server_model = resnet18_model\n",
    "# server_model = BasicNet()\n",
    "\n",
    "# using gpu for computations if available\n",
    "server_model = server_model.to(device)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# choose how many clients you want send model to\n",
    "client_frac = 0.3\n",
    "total_clients = 10\n",
    "client_training_losses = [ [] for i in range(total_clients)]\n",
    "client_training_accs = [ [] for i in range(total_clients)]\n",
    "client_weights = [1/total_clients for i in range(total_clients)] # need to check about this\n",
    "client_models = [copy.deepcopy(server_model).to(device) for idx in range(total_clients)]\n",
    "\n",
    "fed_rounds = 12\n",
    "local_epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "# Save the tensor of images and labels for clients\n",
    "username = 'fnx11'\n",
    "data_folder = f'/home/{username}/thesis/codes/Playground/data/fed_data/'\n",
    "logs_folder = f'/home/{username}/thesis/codes/Playground/logs/'\n",
    "writer = SummaryWriter(logs_folder+'fed_cifar10_experiment')\n",
    "\n",
    "# specify learning rate to be used\n",
    "learning_rate = 0.01 # change this according to our model, tranfer learning use 0.001, basic model use 0.01\n",
    "optimizers = [optim.SGD(params=client_models[idx].parameters(), lr=learning_rate) for idx in range(total_clients)]\n",
    "client_data_loaders = create_client_data_loaders(total_clients, data_folder, batch_size)\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "data_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.5, 0.5, 0.5],[0.5, 0.5, 0.5])])\n",
    "testset = datasets.CIFAR10('~/.pytorch/CIFAR10_data/', train=False, download=True, transform=data_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c250df33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 9 6]\n",
      "Client: 0\t Epoch: 0 \tTraining Loss: 1.811408\n",
      "Client: 0\t Epoch: 1 \tTraining Loss: 1.793063\n",
      "Client: 0\t Epoch: 2 \tTraining Loss: 1.770396\n",
      "Client: 9\t Epoch: 0 \tTraining Loss: 1.818543\n",
      "Client: 9\t Epoch: 1 \tTraining Loss: 1.797690\n",
      "Client: 9\t Epoch: 2 \tTraining Loss: 1.776153\n",
      "Client: 6\t Epoch: 0 \tTraining Loss: 1.802555\n",
      "Client: 6\t Epoch: 1 \tTraining Loss: 1.777556\n",
      "Client: 6\t Epoch: 2 \tTraining Loss: 1.754153\n",
      "Round 0 complete\n",
      "[4 8 7]\n",
      "Client: 4\t Epoch: 0 \tTraining Loss: 1.816725\n",
      "Client: 4\t Epoch: 1 \tTraining Loss: 1.795713\n",
      "Client: 4\t Epoch: 2 \tTraining Loss: 1.772180\n",
      "Client: 8\t Epoch: 0 \tTraining Loss: 1.816946\n",
      "Client: 8\t Epoch: 1 \tTraining Loss: 1.798205\n",
      "Client: 8\t Epoch: 2 \tTraining Loss: 1.776107\n",
      "Client: 7\t Epoch: 0 \tTraining Loss: 1.796422\n",
      "Client: 7\t Epoch: 1 \tTraining Loss: 1.770936\n",
      "Client: 7\t Epoch: 2 \tTraining Loss: 1.747796\n",
      "Round 1 complete\n",
      "[4 6 1]\n",
      "Client: 4\t Epoch: 0 \tTraining Loss: 1.809625\n",
      "Client: 4\t Epoch: 1 \tTraining Loss: 1.788172\n",
      "Client: 4\t Epoch: 2 \tTraining Loss: 1.764984\n",
      "Client: 6\t Epoch: 0 \tTraining Loss: 1.793812\n",
      "Client: 6\t Epoch: 1 \tTraining Loss: 1.768868\n",
      "Client: 6\t Epoch: 2 \tTraining Loss: 1.746699\n",
      "Client: 1\t Epoch: 0 \tTraining Loss: 1.812108\n",
      "Client: 1\t Epoch: 1 \tTraining Loss: 1.791196\n",
      "Client: 1\t Epoch: 2 \tTraining Loss: 1.768112\n",
      "Round 2 complete\n",
      "[4 3 1]\n",
      "Client: 4\t Epoch: 0 \tTraining Loss: 1.801592\n",
      "Client: 4\t Epoch: 1 \tTraining Loss: 1.780794\n",
      "Client: 4\t Epoch: 2 \tTraining Loss: 1.757606\n",
      "Client: 3\t Epoch: 0 \tTraining Loss: 1.807077\n",
      "Client: 3\t Epoch: 1 \tTraining Loss: 1.784226\n",
      "Client: 3\t Epoch: 2 \tTraining Loss: 1.761032\n",
      "Client: 1\t Epoch: 0 \tTraining Loss: 1.804019\n",
      "Client: 1\t Epoch: 1 \tTraining Loss: 1.783469\n",
      "Client: 1\t Epoch: 2 \tTraining Loss: 1.761224\n",
      "Round 3 complete\n",
      "Test Loss: 0.075301\n",
      "\n",
      "Test Accuracy of airplane: 100% (1000/1000)\n",
      "Test Accuracy of automobile:  0% ( 0/1000)\n",
      "Test Accuracy of  bird:  0% ( 0/1000)\n",
      "Test Accuracy of   cat:  0% ( 0/1000)\n",
      "Test Accuracy of  deer:  0% ( 0/1000)\n",
      "Test Accuracy of   dog:  0% ( 0/1000)\n",
      "Test Accuracy of  frog:  0% ( 0/1000)\n",
      "Test Accuracy of horse:  0% ( 0/1000)\n",
      "Test Accuracy of  ship:  0% ( 0/1000)\n",
      "Test Accuracy of truck:  0% ( 0/1000)\n",
      "\n",
      "Test Accuracy (Overall): 10% (1000/10000)\n",
      "[8 5 3]\n",
      "Client: 8\t Epoch: 0 \tTraining Loss: 1.807012\n",
      "Client: 8\t Epoch: 1 \tTraining Loss: 1.789150\n",
      "Client: 8\t Epoch: 2 \tTraining Loss: 1.766308\n",
      "Client: 5\t Epoch: 0 \tTraining Loss: 1.829977\n",
      "Client: 5\t Epoch: 1 \tTraining Loss: 1.814667\n",
      "Client: 5\t Epoch: 2 \tTraining Loss: 1.790654\n",
      "Client: 3\t Epoch: 0 \tTraining Loss: 1.800114\n",
      "Client: 3\t Epoch: 1 \tTraining Loss: 1.776504\n",
      "Client: 3\t Epoch: 2 \tTraining Loss: 1.754593\n",
      "Round 4 complete\n",
      "[3 0 2]\n",
      "Client: 3\t Epoch: 0 \tTraining Loss: 1.793986\n",
      "Client: 3\t Epoch: 1 \tTraining Loss: 1.769065\n",
      "Client: 3\t Epoch: 2 \tTraining Loss: 1.747425\n",
      "Client: 0\t Epoch: 0 \tTraining Loss: 1.799033\n",
      "Client: 0\t Epoch: 1 \tTraining Loss: 1.780084\n",
      "Client: 0\t Epoch: 2 \tTraining Loss: 1.757788\n",
      "Client: 2\t Epoch: 0 \tTraining Loss: 1.814138\n",
      "Client: 2\t Epoch: 1 \tTraining Loss: 1.794092\n",
      "Client: 2\t Epoch: 2 \tTraining Loss: 1.770275\n",
      "Round 5 complete\n",
      "[8 7 0]\n",
      "Client: 8\t Epoch: 0 \tTraining Loss: 1.799960\n",
      "Client: 8\t Epoch: 1 \tTraining Loss: 1.781347\n",
      "Client: 8\t Epoch: 2 \tTraining Loss: 1.758011\n",
      "Client: 7\t Epoch: 0 \tTraining Loss: 1.785303\n",
      "Client: 7\t Epoch: 1 \tTraining Loss: 1.758733\n",
      "Client: 7\t Epoch: 2 \tTraining Loss: 1.736251\n",
      "Client: 0\t Epoch: 0 \tTraining Loss: 1.791590\n",
      "Client: 0\t Epoch: 1 \tTraining Loss: 1.772691\n",
      "Client: 0\t Epoch: 2 \tTraining Loss: 1.749955\n",
      "Round 6 complete\n",
      "[7 1 6]\n",
      "Client: 7\t Epoch: 0 \tTraining Loss: 1.778248\n",
      "Client: 7\t Epoch: 1 \tTraining Loss: 1.751359\n",
      "Client: 7\t Epoch: 2 \tTraining Loss: 1.729178\n",
      "Client: 1\t Epoch: 0 \tTraining Loss: 1.794779\n",
      "Client: 1\t Epoch: 1 \tTraining Loss: 1.773864\n",
      "Client: 1\t Epoch: 2 \tTraining Loss: 1.752142\n",
      "Client: 6\t Epoch: 0 \tTraining Loss: 1.782407\n",
      "Client: 6\t Epoch: 1 \tTraining Loss: 1.759035\n",
      "Client: 6\t Epoch: 2 \tTraining Loss: 1.735379\n",
      "Round 7 complete\n",
      "Test Loss: 0.075332\n",
      "\n",
      "Test Accuracy of airplane: 100% (1000/1000)\n",
      "Test Accuracy of automobile:  0% ( 0/1000)\n",
      "Test Accuracy of  bird:  0% ( 0/1000)\n",
      "Test Accuracy of   cat:  0% ( 0/1000)\n",
      "Test Accuracy of  deer:  0% ( 0/1000)\n",
      "Test Accuracy of   dog:  0% ( 0/1000)\n",
      "Test Accuracy of  frog:  0% ( 0/1000)\n",
      "Test Accuracy of horse:  0% ( 0/1000)\n",
      "Test Accuracy of  ship:  0% ( 0/1000)\n",
      "Test Accuracy of truck:  0% ( 0/1000)\n",
      "\n",
      "Test Accuracy (Overall): 10% (1000/10000)\n",
      "[7 4 8]\n",
      "Client: 7\t Epoch: 0 \tTraining Loss: 1.769821\n",
      "Client: 7\t Epoch: 1 \tTraining Loss: 1.743533\n",
      "Client: 7\t Epoch: 2 \tTraining Loss: 1.721489\n",
      "Client: 4\t Epoch: 0 \tTraining Loss: 1.792803\n",
      "Client: 4\t Epoch: 1 \tTraining Loss: 1.771161\n",
      "Client: 4\t Epoch: 2 \tTraining Loss: 1.748816\n",
      "Client: 8\t Epoch: 0 \tTraining Loss: 1.790401\n",
      "Client: 8\t Epoch: 1 \tTraining Loss: 1.771951\n",
      "Client: 8\t Epoch: 2 \tTraining Loss: 1.749669\n",
      "Round 8 complete\n",
      "[3 6 1]\n",
      "Client: 3\t Epoch: 0 \tTraining Loss: 1.784794\n",
      "Client: 3\t Epoch: 1 \tTraining Loss: 1.760970\n",
      "Client: 3\t Epoch: 2 \tTraining Loss: 1.738876\n",
      "Client: 6\t Epoch: 0 \tTraining Loss: 1.773336\n",
      "Client: 6\t Epoch: 1 \tTraining Loss: 1.750523\n",
      "Client: 6\t Epoch: 2 \tTraining Loss: 1.726589\n",
      "Client: 1\t Epoch: 0 \tTraining Loss: 1.785407\n",
      "Client: 1\t Epoch: 1 \tTraining Loss: 1.765953\n",
      "Client: 1\t Epoch: 2 \tTraining Loss: 1.744549\n",
      "Round 9 complete\n",
      "[0 4 1]\n",
      "Client: 0\t Epoch: 0 \tTraining Loss: 1.780145\n",
      "Client: 0\t Epoch: 1 \tTraining Loss: 1.762023\n",
      "Client: 0\t Epoch: 2 \tTraining Loss: 1.740364\n",
      "Client: 4\t Epoch: 0 \tTraining Loss: 1.784433\n",
      "Client: 4\t Epoch: 1 \tTraining Loss: 1.764073\n",
      "Client: 4\t Epoch: 2 \tTraining Loss: 1.741317\n",
      "Client: 1\t Epoch: 0 \tTraining Loss: 1.777602\n",
      "Client: 1\t Epoch: 1 \tTraining Loss: 1.759297\n",
      "Client: 1\t Epoch: 2 \tTraining Loss: 1.737028\n",
      "Round 10 complete\n",
      "[6 7 3]\n",
      "Client: 6\t Epoch: 0 \tTraining Loss: 1.765040\n",
      "Client: 6\t Epoch: 1 \tTraining Loss: 1.742139\n",
      "Client: 6\t Epoch: 2 \tTraining Loss: 1.718548\n",
      "Client: 7\t Epoch: 0 \tTraining Loss: 1.761350\n",
      "Client: 7\t Epoch: 1 \tTraining Loss: 1.735408\n",
      "Client: 7\t Epoch: 2 \tTraining Loss: 1.712812\n",
      "Client: 3\t Epoch: 0 \tTraining Loss: 1.777580\n",
      "Client: 3\t Epoch: 1 \tTraining Loss: 1.754299\n",
      "Client: 3\t Epoch: 2 \tTraining Loss: 1.731258\n",
      "Round 11 complete\n",
      "Test Loss: 0.075273\n",
      "\n",
      "Test Accuracy of airplane: 100% (1000/1000)\n",
      "Test Accuracy of automobile:  0% ( 0/1000)\n",
      "Test Accuracy of  bird:  0% ( 0/1000)\n",
      "Test Accuracy of   cat:  0% ( 0/1000)\n",
      "Test Accuracy of  deer:  0% ( 0/1000)\n",
      "Test Accuracy of   dog:  0% ( 0/1000)\n",
      "Test Accuracy of  frog:  0% ( 0/1000)\n",
      "Test Accuracy of horse:  0% ( 0/1000)\n",
      "Test Accuracy of  ship:  0% ( 0/1000)\n",
      "Test Accuracy of truck:  0% ( 0/1000)\n",
      "\n",
      "Test Accuracy (Overall): 10% (1000/10000)\n"
     ]
    }
   ],
   "source": [
    "###Training and testing model every kth round\n",
    "k = 4\n",
    "testing_losses = []\n",
    "testing_accs = []\n",
    "for i in range(fed_rounds):\n",
    "    clients_selected = rng.choice(total_clients, size=int(total_clients*client_frac), replace=False)\n",
    "    print(clients_selected)\n",
    "    for j in clients_selected:\n",
    "        training_losses = train_on_client(j, client_models[j], client_data_loaders[j], optimizers[j], loss_fn, local_epochs, device)\n",
    "        client_training_losses[j].extend(training_losses)\n",
    "#         client_training_accs[j].extend(training_accs)\n",
    "    # aggregate to update server_model and client_models\n",
    "    print(f\"Round {i} complete\")\n",
    "    server_model, client_models = fed_avg(server_model, client_models, client_weights)\n",
    "    # Testing Model every kth round\n",
    "    if (i+1)%k==0:\n",
    "        test_loss, test_acc = run_test(server_model, test_loader, loss_fn, device)\n",
    "        testing_losses.append(test_loss)\n",
    "        testing_accs.append(test_acc)\n",
    "\n",
    "###Logging of losses\n",
    "for i in range(total_clients):\n",
    "    for j in range(len(client_training_losses[i])):\n",
    "        writer.add_scalar(f'training_loss_client_{i}', client_training_losses[i][j], j+1)\n",
    "\n",
    "###Logging of Testing losses and testing accs\n",
    "for e in range(len(testing_losses)):\n",
    "    writer.add_scalar('testing loss', testing_losses[e], e+1)\n",
    "    writer.add_scalar('model accuracy', testing_accs[e], e+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a22651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
